---
title: "DAP II Problem Set 5"
author: Sara Van Valkenburgh and Jennifer Edouard
date: November 9, 2025
geometry: 
    margin=0.75in
fontsize: 10pt
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Sara Van Valkenburgh, vanvals
    - Partner 2 (name and cnet ID): Jennifer Edouard, jkedouard
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*sv\*\* \*\*je\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)

soup = BeautifulSoup(response.text, 'html.parser')
```

```{python}
# Empty lists to store the information
titles = []
dates = []
categories = []
links = []
```

```{python}
# Title of the enforcement action
for heading in soup.find_all('h2', class_='usa-card__heading'):
    title = heading.find('a').get_text(strip=True)
    titles.append(title)
```

```{python}
# Date
for date_tag in soup.find_all('span', class_='text-base-dark padding-right-105'):
    date = date_tag.get_text(strip=True)
    dates.append(date)
```

```{python}
# Category
for ul in soup.find_all('ul', class_='display-inline add-list-reset'):
    for li in ul.find_all('li'):
        category = li.get_text(strip=True)
        categories.append(category)
```

```{python}
# Link associated with enforcement action
for action in soup.find_all('a', href=True):
    href = action['href']
    if href.startswith("/fraud/enforcement/"):
        full_link = f"https://oig.hhs.gov{href}"
        links.append(full_link)

# Remove the first three links; they are irrelevant
links = links[3:] 
```

 ```{python}
# Put into a dataframe
 data = {
     'Title': titles,
     'Date': dates,
     'Category': categories,
     'Link': links
}
df = pd.DataFrame(data)
df.head()
 ``` 

### 2. Crawling (PARTNER 1)

```{python}
def get_agency_name(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    agency_name = "N/A"
    agency_label = soup.find(
        'span', class_='padding-right-2 text-base', string="Agency:")

    if agency_label and agency_label.next_sibling:
        agency_name = agency_label.next_sibling.get_text(strip=True)

    return agency_name
```

```{python}
data = {
    'Title': titles,
    'Date': dates,
    'Category': categories,
    'Link': links
}
df = pd.DataFrame(data)
df['Agency'] = None

for idx, link in enumerate(df['Link']):
    df.at[idx, 'Agency'] = get_agency_name(link)

df.head()
```

Source: https://stackoverflow.com/questions/16476924/how-can-i-iterate-over-rows-in-a-pandas-dataframe

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

```{python}

from datetime import datetime

def scrape_enforcement_actions(year, month):
    # make it restart if a year lower than 2013 is entered
    if year < 2013:
        print("Please enter a year greater than or equal to 2013.")
        return

    # same url opening for all the pages scraped
    base_url = "https://oig.hhs.gov/fraud/enforcement/"
    all_actions = pd.DataFrame()

    # making sure it stops at today's date (it might do that already bc there's no data past today...)
    start_date = datetime(year, month, 1)
    current_date = datetime.now()
    
    # establishing the pages so it can go on from there
    page_num = 1

    # this just uses the code that Partner 1 wrote, but making it crawl through all the pages
    while True:
        page_url = f"{base_url}?page={page_num}"
        print(f"Scraping page: {page_url}")
        
        response = requests.get(page_url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        titles = [heading.find('a').get_text(strip=True) 
                  for heading in soup.find_all('h2', class_='usa-card__heading')]
        
        date_texts = [date_tag.get_text(strip=True) 
                      for date_tag in soup.find_all('span', class_='text-base-dark padding-right-105')]
        dates = []
        for date_text in date_texts:
            try:
                # Parse the date, handling potential issues with date format
                date = datetime.strptime(date_text, "%B %d, %Y")
                dates.append(date)
            except ValueError:
                print(f"Error parsing date: {date_text}")
                dates.append(None)

        categories = [li.get_text(strip=True) 
                      for ul in soup.find_all('ul', class_='display-inline add-list-reset') 
                      for li in ul.find_all('li')]

        links = [f"https://oig.hhs.gov{a['href']}" 
                 for a in soup.find_all('a', href=True) 
                 if a['href'].startswith("/fraud/enforcement/")]

        # filter based on start and end dates
        filtered_actions = {
            "Title": [],
            "Date": [],
            "Category": [],
            "Link": []
        }
        for i in range(len(dates)):
            # check if the date is valid and within the desired range
            if dates[i] and start_date <= dates[i] <= current_date:
                filtered_actions["Title"].append(titles[i])
                filtered_actions["Date"].append(dates[i].strftime("%Y-%m-%d"))
                filtered_actions["Category"].append(categories[i])
                filtered_actions["Link"].append(links[i])
        
        # add filtered actions to the DataFrame
        page_df = pd.DataFrame(filtered_actions)
        all_actions = pd.concat([all_actions, page_df], ignore_index=True)
        
        # wait before moving to the next page
        time.sleep(1)
        
        # check for the next page
        next_button = soup.find('a', {'class': 'pagination-next'})
        if next_button:
            page_num += 1
        else:
            break

    # save results to CSV
    file_name = f"enforcement_actions_{year}_{month:02d}.csv"
    all_actions.to_csv(file_name, index=False)
    print(f"Data saved to {file_name}")

```

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
# Testing it for January 2023
scrape_enforcement_actions(2023, 1)
```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```